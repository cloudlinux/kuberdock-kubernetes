diff --git a/cmd/kubelet/app/server.go b/cmd/kubelet/app/server.go
index 626cd90..28e406c 100644
--- a/cmd/kubelet/app/server.go
+++ b/cmd/kubelet/app/server.go
@@ -68,6 +68,7 @@ const defaultRootDir = "/var/lib/kubelet"
 // KubeletServer encapsulates all of the parameters necessary for starting up
 // a kubelet. These can either be set via command line or directly.
 type KubeletServer struct {
+	ResourceMultipliers            api.ResourceMultipliers
 	Address                        net.IP
 	AllowPrivileged                bool
 	APIServerList                  []string
@@ -240,6 +241,8 @@ HTTP server: The kubelet can also listen for HTTP and respond to a simple API
 
 // AddFlags adds flags for a specific KubeletServer to the specified FlagSet
 func (s *KubeletServer) AddFlags(fs *pflag.FlagSet) {
+	fs.Float32Var(&s.ResourceMultipliers.CPUMultiplier, "cpu-multiplier", 1.0, "cpu multiplier")
+	fs.Float32Var(&s.ResourceMultipliers.MemoryMultiplier, "memory-multiplier", 1.0, "memory multiplier")
 	fs.StringVar(&s.Config, "config", s.Config, "Path to the config file or directory of files")
 	fs.DurationVar(&s.SyncFrequency, "sync-frequency", s.SyncFrequency, "Max period between synchronizing running containers and config")
 	fs.DurationVar(&s.FileCheckFrequency, "file-check-frequency", s.FileCheckFrequency, "Duration between checking config files for new data")
@@ -377,6 +380,7 @@ func (s *KubeletServer) KubeletConfig() (*KubeletConfig, error) {
 	}
 
 	return &KubeletConfig{
+		ResourceMultipliers:       s.ResourceMultipliers,
 		Address:                   s.Address,
 		AllowPrivileged:           s.AllowPrivileged,
 		CAdvisorInterface:         nil, // launches background processes, not set here
@@ -814,6 +818,7 @@ func makePodSourceConfig(kc *KubeletConfig) *config.PodConfig {
 // KubeletConfig is all of the parameters necessary for running a kubelet.
 // TODO: This should probably be merged with KubeletServer.  The extra object is a consequence of refactoring.
 type KubeletConfig struct {
+	ResourceMultipliers            api.ResourceMultipliers
 	Address                        net.IP
 	AllowPrivileged                bool
 	CAdvisorInterface              cadvisor.Interface
@@ -903,6 +908,7 @@ func createAndInitKubelet(kc *KubeletConfig) (k KubeletBootstrap, pc *config.Pod
 
 	pc = makePodSourceConfig(kc)
 	k, err = kubelet.NewMainKubelet(
+		kc.ResourceMultipliers,
 		kc.Hostname,
 		kc.NodeName,
 		kc.DockerClient,
diff --git a/contrib/mesos/pkg/executor/service/service.go b/contrib/mesos/pkg/executor/service/service.go
index 6c842f9..111b915 100644
--- a/contrib/mesos/pkg/executor/service/service.go
+++ b/contrib/mesos/pkg/executor/service/service.go
@@ -299,6 +299,7 @@ func (ks *KubeletExecutorServer) createAndInitKubelet(
 	updates := pc.Channel(MESOS_CFG_SOURCE)
 
 	klet, err := kubelet.NewMainKubelet(
+		kc.ResourceMultipliers,
 		kc.Hostname,
 		kc.NodeName,
 		kc.DockerClient,
diff --git a/pkg/api/types.go b/pkg/api/types.go
index 3d4184a..3e8375f 100644
--- a/pkg/api/types.go
+++ b/pkg/api/types.go
@@ -1498,6 +1498,11 @@ const (
 	// Number of Pods that may be running on this Node: see ResourcePods
 )
 
+type ResourceMultipliers struct {
+	CPUMultiplier    float32
+	MemoryMultiplier float32
+}
+
 // ResourceList is a set of (resource name, quantity) pairs.
 type ResourceList map[ResourceName]resource.Quantity
 
diff --git a/pkg/api/validation/validation.go b/pkg/api/validation/validation.go
index 1c92a91..286306c 100644
--- a/pkg/api/validation/validation.go
+++ b/pkg/api/validation/validation.go
@@ -1385,9 +1385,9 @@ func ValidatePodTemplateSpecForRC(template *api.PodTemplateSpec, selectorMap map
 			allErrs = append(allErrs, ValidateReadOnlyPersistentDisks(template.Spec.Volumes).Prefix(fieldName+".spec.volumes")...)
 		}
 		// RestartPolicy has already been first-order validated as per ValidatePodTemplateSpec().
-		if template.Spec.RestartPolicy != api.RestartPolicyAlways {
-			allErrs = append(allErrs, errs.NewFieldValueNotSupported(fieldName+".spec.restartPolicy", template.Spec.RestartPolicy, []string{string(api.RestartPolicyAlways)}))
-		}
+		/* if spec.Template.Spec.RestartPolicy != api.RestartPolicyAlways {
+			allErrs = append(allErrs, errs.NewFieldNotSupported("template.restartPolicy", spec.Template.Spec.RestartPolicy))
+		}*/
 	}
 	return allErrs
 }
diff --git a/pkg/controller/controller_utils.go b/pkg/controller/controller_utils.go
index 2601b5b..d0df5f4 100644
--- a/pkg/controller/controller_utils.go
+++ b/pkg/controller/controller_utils.go
@@ -402,3 +402,37 @@ func FilterActivePods(pods []api.Pod) []*api.Pod {
 	}
 	return result
 }
+
+func FilterActiveAndSucceededPods(pods []api.Pod) []*api.Pod {
+	var result []*api.Pod
+	for _, value := range pods {
+		if api.PodFailed != value.Status.Phase {
+			result = append(result, &value)
+		}
+	}
+	return result
+}
+
+func FilterPods(pods []api.Pod) []*api.Pod {
+	var result []*api.Pod
+	if len(pods) == 0 {
+		return result
+	}
+
+	switch pods[0].Spec.RestartPolicy {
+	case api.RestartPolicyAlways:
+		result = FilterActivePods(pods)
+		break
+	case api.RestartPolicyOnFailure:
+		result = FilterActiveAndSucceededPods(pods)
+		break
+	case api.RestartPolicyNever:
+		for _, pod := range pods {
+			result = append(result, &pod)
+		}
+		break
+	default:
+		glog.Errorf("unsupported restart policy %v\n", pods[0].Spec.RestartPolicy)
+	}
+	return result
+}
diff --git a/pkg/controller/replication/replication_controller.go b/pkg/controller/replication/replication_controller.go
index 6cf0ef8..bad7cc1 100644
--- a/pkg/controller/replication/replication_controller.go
+++ b/pkg/controller/replication/replication_controller.go
@@ -435,7 +435,7 @@ func (rm *ReplicationManager) syncReplicationController(key string) error {
 	}
 
 	// TODO: Do this in a single pass, or use an index.
-	filteredPods := controller.FilterActivePods(podList.Items)
+	filteredPods := controller.FilterPods(podList.Items)
 	if rcNeedsSync {
 		rm.manageReplicas(filteredPods, &rc)
 	}
diff --git a/pkg/controller/replication/replication_controller_test.go b/pkg/controller/replication/replication_controller_test.go
index 48b8665..4147c68 100644
--- a/pkg/controller/replication/replication_controller_test.go
+++ b/pkg/controller/replication/replication_controller_test.go
@@ -52,7 +52,7 @@ func getKey(rc *api.ReplicationController, t *testing.T) string {
 	}
 }
 
-func newReplicationController(replicas int) *api.ReplicationController {
+func newReplicationController(replicas int, restartPolicy api.RestartPolicy) *api.ReplicationController {
 	rc := &api.ReplicationController{
 		TypeMeta: unversioned.TypeMeta{APIVersion: testapi.Default.Version()},
 		ObjectMeta: api.ObjectMeta{
@@ -80,7 +80,7 @@ func newReplicationController(replicas int) *api.ReplicationController {
 							SecurityContext:        securitycontext.ValidSecurityContextWithContainerDefaults(),
 						},
 					},
-					RestartPolicy: api.RestartPolicyAlways,
+					RestartPolicy: restartPolicy,
 					DNSPolicy:     api.DNSDefault,
 					NodeSelector: map[string]string{
 						"baz": "blah",
@@ -132,14 +132,14 @@ type serverResponse struct {
 	obj        interface{}
 }
 
-func TestSyncReplicationControllerDoesNothing(t *testing.T) {
+func TestSyncReplicationControllerAlwaysDoesNothing(t *testing.T) {
 	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
 	fakePodControl := controller.FakePodControl{}
 	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
 	manager.podStoreSynced = alwaysReady
 
 	// 2 running pods, a controller with 2 replicas, sync is a no-op
-	controllerSpec := newReplicationController(2)
+	controllerSpec := newReplicationController(2, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
 
@@ -148,7 +148,39 @@ func TestSyncReplicationControllerDoesNothing(t *testing.T) {
 	validateSyncReplication(t, &fakePodControl, 0, 0)
 }
 
-func TestSyncReplicationControllerDeletes(t *testing.T) {
+func TestSyncReplicationControllerOnFailureDoesNothing(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	fakePodControl := controller.FakePodControl{}
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+
+	// 2 running pods, a controller with 2 replicas, sync is a no-op
+	controllerSpec := newReplicationController(2, api.RestartPolicyOnFailure)
+	manager.rcStore.Store.Add(controllerSpec)
+	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
+
+	manager.podControl = &fakePodControl
+	manager.syncReplicationController(getKey(controllerSpec, t))
+	validateSyncReplication(t, &fakePodControl, 0, 0)
+}
+
+func TestSyncReplicationControllerNeverDoesNothing(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	fakePodControl := controller.FakePodControl{}
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+
+	// 2 failed pods, a controller with 2 replicas, sync is a no-op
+	controllerSpec := newReplicationController(2, api.RestartPolicyNever)
+	manager.rcStore.Store.Add(controllerSpec)
+	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
+
+	manager.podControl = &fakePodControl
+	manager.syncReplicationController(getKey(controllerSpec, t))
+	validateSyncReplication(t, &fakePodControl, 0, 0)
+}
+
+func TestSyncReplicationControllerAlwaysDeletes(t *testing.T) {
 	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
 	fakePodControl := controller.FakePodControl{}
 	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
@@ -156,7 +188,39 @@ func TestSyncReplicationControllerDeletes(t *testing.T) {
 	manager.podControl = &fakePodControl
 
 	// 2 running pods and a controller with 1 replica, one pod delete expected
-	controllerSpec := newReplicationController(1)
+	controllerSpec := newReplicationController(1, api.RestartPolicyAlways)
+	manager.rcStore.Store.Add(controllerSpec)
+	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
+
+	manager.syncReplicationController(getKey(controllerSpec, t))
+	validateSyncReplication(t, &fakePodControl, 0, 1)
+}
+
+func TestSyncReplicationControllerOnFailureDeletes(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	fakePodControl := controller.FakePodControl{}
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+	manager.podControl = &fakePodControl
+
+	// 2 running pods and a controller with 1 replica, one pod delete expected
+	controllerSpec := newReplicationController(1, api.RestartPolicyOnFailure)
+	manager.rcStore.Store.Add(controllerSpec)
+	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
+
+	manager.syncReplicationController(getKey(controllerSpec, t))
+	validateSyncReplication(t, &fakePodControl, 0, 1)
+}
+
+func TestSyncReplicationControllerNeverDeletes(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	fakePodControl := controller.FakePodControl{}
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+	manager.podControl = &fakePodControl
+
+	// 2 running pods and a controller with 1 replica, one pod delete expected
+	controllerSpec := newReplicationController(1, api.RestartPolicyNever)
 	manager.rcStore.Store.Add(controllerSpec)
 	newPodList(manager.podStore.Store, 2, api.PodRunning, controllerSpec)
 
@@ -179,7 +243,7 @@ func TestDeleteFinalStateUnknown(t *testing.T) {
 
 	// The DeletedFinalStateUnknown object should cause the rc manager to insert
 	// the controller matching the selectors of the deleted pod into the work queue.
-	controllerSpec := newReplicationController(1)
+	controllerSpec := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 	pods := newPodList(nil, 1, api.PodRunning, controllerSpec)
 	manager.deletePod(cache.DeletedFinalStateUnknown{Key: "foo", Obj: &pods.Items[0]})
@@ -197,14 +261,46 @@ func TestDeleteFinalStateUnknown(t *testing.T) {
 	}
 }
 
-func TestSyncReplicationControllerCreates(t *testing.T) {
+func TestSyncReplicationControllerAlwaysCreates(t *testing.T) {
 	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
 	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
 	manager.podStoreSynced = alwaysReady
 
 	// A controller with 2 replicas and no pods in the store, 2 creates expected
-	rc := newReplicationController(2)
+	rc := newReplicationController(2, api.RestartPolicyAlways)
+	manager.rcStore.Store.Add(rc)
+
+	fakePodControl := controller.FakePodControl{}
+	manager.podControl = &fakePodControl
+	manager.syncReplicationController(getKey(rc, t))
+	validateSyncReplication(t, &fakePodControl, 2, 0)
+}
+
+func TestSyncReplicationControllerOnFailureCreates(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+
+	// A controller with 2 replicas and one failed pod in the store, 2 creates expected
+	rc := newReplicationController(2, api.RestartPolicyOnFailure)
+	manager.rcStore.Store.Add(rc)
+	newPodList(manager.podStore.Store, 1, api.PodFailed, controllerSpec)
+
+	fakePodControl := controller.FakePodControl{}
+	manager.podControl = &fakePodControl
+	manager.syncReplicationController(getKey(rc, t))
+	validateSyncReplication(t, &fakePodControl, 2, 0)
+}
+
+func TestSyncReplicationControllerNeverCreates(t *testing.T) {
+	client := client.NewOrDie(&client.Config{Host: "", Version: testapi.Default.Version()})
+	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
+	manager.podStoreSynced = alwaysReady
+
+	// A controller with 2 replicas and one failed pod in the store, 1 creates expected
+	rc := newReplicationController(2, api.RestartPolicyOnFailure)
 	manager.rcStore.Store.Add(rc)
+	newPodList(manager.podStore.Store, 1, api.PodFailed, controllerSpec)
 
 	fakePodControl := controller.FakePodControl{}
 	manager.podControl = &fakePodControl
@@ -226,7 +322,7 @@ func TestStatusUpdatesWithoutReplicasChange(t *testing.T) {
 
 	// Steady state for the replication controller, no Status.Replicas updates expected
 	activePods := 5
-	rc := newReplicationController(activePods)
+	rc := newReplicationController(activePods, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(rc)
 	rc.Status = api.ReplicationControllerStatus{Replicas: activePods}
 	newPodList(manager.podStore.Store, activePods, api.PodRunning, rc)
@@ -268,7 +364,7 @@ func TestControllerUpdateReplicas(t *testing.T) {
 
 	// Insufficient number of pods in the system, and Status.Replicas is wrong;
 	// Status.Replica should update to match number of pods in system, 1 new pod should be created.
-	rc := newReplicationController(5)
+	rc := newReplicationController(5, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(rc)
 	rc.Status = api.ReplicationControllerStatus{Replicas: 2, ObservedGeneration: 0}
 	rc.Generation = 1
@@ -307,7 +403,7 @@ func TestSyncReplicationControllerDormancy(t *testing.T) {
 	manager.podStoreSynced = alwaysReady
 	manager.podControl = &fakePodControl
 
-	controllerSpec := newReplicationController(2)
+	controllerSpec := newReplicationController(2, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 	newPodList(manager.podStore.Store, 1, api.PodRunning, controllerSpec)
 
@@ -464,7 +560,7 @@ func TestWatchPods(t *testing.T) {
 	manager.podStoreSynced = alwaysReady
 
 	// Put one rc and one pod into the controller's stores
-	testControllerSpec := newReplicationController(1)
+	testControllerSpec := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(testControllerSpec)
 	received := make(chan string)
 	// The pod update sent through the fakeWatcher should figure out the managing rc and
@@ -524,7 +620,7 @@ func TestUpdatePods(t *testing.T) {
 	go util.Until(manager.worker, 10*time.Millisecond, stopCh)
 
 	// Put 2 rcs and one pod into the controller's stores
-	testControllerSpec1 := newReplicationController(1)
+	testControllerSpec1 := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(testControllerSpec1)
 	testControllerSpec2 := *testControllerSpec1
 	testControllerSpec2.Spec.Selector = map[string]string{"bar": "foo"}
@@ -567,7 +663,7 @@ func TestControllerUpdateRequeue(t *testing.T) {
 	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, BurstReplicas)
 	manager.podStoreSynced = alwaysReady
 
-	rc := newReplicationController(1)
+	rc := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(rc)
 	rc.Status = api.ReplicationControllerStatus{Replicas: 2}
 	newPodList(manager.podStore.Store, 1, api.PodRunning, rc)
@@ -597,7 +693,7 @@ func TestControllerUpdateRequeue(t *testing.T) {
 }
 
 func TestControllerUpdateStatusWithFailure(t *testing.T) {
-	rc := newReplicationController(1)
+	rc := newReplicationController(1, api.RestartPolicyAlways)
 	fakeClient := &testclient.Fake{}
 	fakeClient.AddReactor("get", "replicationcontrollers", func(action testclient.Action) (bool, runtime.Object, error) {
 		return true, rc, nil
@@ -649,7 +745,7 @@ func doTestControllerBurstReplicas(t *testing.T, burstReplicas, numReplicas int)
 	manager.podStoreSynced = alwaysReady
 	manager.podControl = &fakePodControl
 
-	controllerSpec := newReplicationController(numReplicas)
+	controllerSpec := newReplicationController(numReplicas, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 
 	expectedPods := 0
@@ -769,7 +865,7 @@ func TestRCSyncExpectations(t *testing.T) {
 	manager.podStoreSynced = alwaysReady
 	manager.podControl = &fakePodControl
 
-	controllerSpec := newReplicationController(2)
+	controllerSpec := newReplicationController(2, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 	pods := newPodList(nil, 2, api.PodPending, controllerSpec)
 	manager.podStore.Store.Add(&pods.Items[0])
@@ -792,7 +888,7 @@ func TestDeleteControllerAndExpectations(t *testing.T) {
 	manager := NewReplicationManager(client, controller.NoResyncPeriodFunc, 10)
 	manager.podStoreSynced = alwaysReady
 
-	rc := newReplicationController(1)
+	rc := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(rc)
 
 	fakePodControl := controller.FakePodControl{}
@@ -839,7 +935,7 @@ func TestRCManagerNotReady(t *testing.T) {
 	// Simulates the rc reflector running before the pod reflector. We don't
 	// want to end up creating replicas in this case until the pod reflector
 	// has synced, so the rc manager should just requeue the rc.
-	controllerSpec := newReplicationController(1)
+	controllerSpec := newReplicationController(1, api.RestartPolicyAlways)
 	manager.rcStore.Store.Add(controllerSpec)
 
 	rcKey := getKey(controllerSpec, t)
@@ -876,7 +972,7 @@ func TestOverlappingRCs(t *testing.T) {
 		// Create 10 rcs, shuffled them randomly and insert them into the rc manager's store
 		var controllers []*api.ReplicationController
 		for j := 1; j < 10; j++ {
-			controllerSpec := newReplicationController(1)
+			controllerSpec := newReplicationController(1, api.RestartPolicyAlways)
 			controllerSpec.CreationTimestamp = unversioned.Date(2014, time.December, j, 0, 0, 0, 0, time.Local)
 			controllerSpec.Name = string(util.NewUUID())
 			controllers = append(controllers, controllerSpec)
diff --git a/pkg/kubelet/container_manager_linux.go b/pkg/kubelet/container_manager_linux.go
index 4a9ac29..113351b 100644
--- a/pkg/kubelet/container_manager_linux.go
+++ b/pkg/kubelet/container_manager_linux.go
@@ -80,8 +80,9 @@ type nodeConfig struct {
 }
 
 type containerManagerImpl struct {
-	cadvisorInterface cadvisor.Interface
-	mountUtil         mount.Interface
+	resourceMultipliers api.ResourceMultipliers
+	cadvisorInterface   cadvisor.Interface
+	mountUtil           mount.Interface
 	nodeConfig
 	// External containers being managed.
 	systemContainers []*systemContainer
@@ -120,10 +121,11 @@ func validateSystemRequirements(mountUtil mount.Interface) error {
 // TODO(vmarmol): Add limits to the system containers.
 // Takes the absolute name of the specified containers.
 // Empty container name disables use of the specified container.
-func newContainerManager(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, dockerDaemonContainerName, systemContainerName, kubeletContainerName string) (containerManager, error) {
+func newContainerManager(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, dockerDaemonContainerName, systemContainerName, kubeletContainerName string, resourceMultipliers api.ResourceMultipliers) (containerManager, error) {
 	return &containerManagerImpl{
-		cadvisorInterface: cadvisorInterface,
-		mountUtil:         mountUtil,
+		resourceMultipliers: resourceMultipliers,
+		cadvisorInterface:   cadvisorInterface,
+		mountUtil:           mountUtil,
 		nodeConfig: nodeConfig{
 			dockerDaemonContainerName: dockerDaemonContainerName,
 			systemContainerName:       systemContainerName,
@@ -204,8 +206,9 @@ func (cm *containerManagerImpl) setupNode() error {
 		var capacity = api.ResourceList{}
 		if err != nil {
 		} else {
-			capacity = CapacityFromMachineInfo(info)
+			capacity = CapacityFromMachineInfo(info, cm.resourceMultipliers)
 		}
+		glog.V(4).Infof("Container manager capacity: %v\n", capacity)
 		memoryLimit := (int64(capacity.Memory().Value() * DockerMemoryLimitThresholdPercent / 100))
 		if memoryLimit < MinDockerMemoryLimit {
 			glog.Warningf("Memory limit %d for container %s is too small, reset it to %d", memoryLimit, cm.dockerDaemonContainerName, MinDockerMemoryLimit)
diff --git a/pkg/kubelet/container_manager_unsupported.go b/pkg/kubelet/container_manager_unsupported.go
index c18e679..8669e74 100644
--- a/pkg/kubelet/container_manager_unsupported.go
+++ b/pkg/kubelet/container_manager_unsupported.go
@@ -39,6 +39,6 @@ func (unsupportedContainerManager) SystemContainersLimit() api.ResourceList {
 	return api.ResourceList{}
 }
 
-func newContainerManager(mounter mount.Interface, cadvisorInterface cadvisor.Interface, dockerDaemonContainer, systemContainer, kubeletContainer string) (containerManager, error) {
+func newContainerManager(mounter mount.Interface, cadvisorInterface cadvisor.Interface, dockerDaemonContainer, systemContainer, kubeletContainer string, resourceMultipliers api.ResourceMultipliers) (containerManager, error) {
 	return &unsupportedContainerManager{}, nil
 }
diff --git a/pkg/kubelet/kubelet.go b/pkg/kubelet/kubelet.go
index a80673c..f4db1b2 100644
--- a/pkg/kubelet/kubelet.go
+++ b/pkg/kubelet/kubelet.go
@@ -137,6 +137,7 @@ func waitUntilRuntimeIsUp(cr kubecontainer.Runtime, timeout time.Duration) error
 
 // New creates a new Kubelet for use in main
 func NewMainKubelet(
+	resourceMultipliers api.ResourceMultipliers,
 	hostname string,
 	nodeName string,
 	dockerClient dockertools.DockerInterface,
@@ -259,6 +260,7 @@ func NewMainKubelet(
 	oomWatcher := NewOOMWatcher(cadvisorInterface, recorder)
 
 	klet := &Kubelet{
+		resourceMultipliers:            resourceMultipliers,
 		hostname:                       hostname,
 		nodeName:                       nodeName,
 		dockerClient:                   dockerClient,
@@ -372,7 +374,7 @@ func NewMainKubelet(
 
 	// Setup container manager, can fail if the devices hierarchy is not mounted
 	// (it is required by Docker however).
-	containerManager, err := newContainerManager(mounter, cadvisorInterface, dockerDaemonContainer, systemContainer, resourceContainer)
+	containerManager, err := newContainerManager(mounter, cadvisorInterface, dockerDaemonContainer, systemContainer, resourceContainer, resourceMultipliers)
 	if err != nil {
 		return nil, fmt.Errorf("failed to create the Container Manager: %v", err)
 	}
@@ -419,6 +421,8 @@ func NewMainKubelet(
 	klet.backOff = util.NewBackOff(resyncInterval, maxContainerBackOff)
 	klet.podKillingCh = make(chan *kubecontainer.Pod, podKillingChannelCapacity)
 
+	glog.V(4).Infof("Kubelet resource multipliers: %v\n", klet.resourceMultipliers)
+
 	return klet, nil
 }
 
@@ -433,16 +437,17 @@ type nodeLister interface {
 
 // Kubelet is the main kubelet implementation.
 type Kubelet struct {
-	hostname       string
-	nodeName       string
-	dockerClient   dockertools.DockerInterface
-	runtimeCache   kubecontainer.RuntimeCache
-	kubeClient     client.Interface
-	rootDirectory  string
-	podWorkers     PodWorkers
-	resyncInterval time.Duration
-	resyncTicker   *time.Ticker
-	sourcesReady   SourcesReadyFn
+	resourceMultipliers api.ResourceMultipliers
+	hostname            string
+	nodeName            string
+	dockerClient        dockertools.DockerInterface
+	runtimeCache        kubecontainer.RuntimeCache
+	kubeClient          client.Interface
+	rootDirectory       string
+	podWorkers          PodWorkers
+	resyncInterval      time.Duration
+	resyncTicker        *time.Ticker
+	sourcesReady        SourcesReadyFn
 
 	podManager podManager
 
@@ -1876,7 +1881,8 @@ func (kl *Kubelet) hasInsufficientfFreeResources(pods []*api.Pod) (bool, bool) {
 		// TODO: Should we admit the pod when machine info is unavailable?
 		return false, false
 	}
-	capacity := CapacityFromMachineInfo(info)
+	capacity := CapacityFromMachineInfo(info, kl.resourceMultipliers)
+	glog.V(4).Infof("Kubelet capacity: %v\n", capacity)
 	_, notFittingCPU, notFittingMemory := predicates.CheckPodsExceedingFreeResources(pods, capacity)
 	return len(notFittingCPU) > 0, len(notFittingMemory) > 0
 }
@@ -2412,7 +2418,7 @@ func (kl *Kubelet) setNodeStatus(node *api.Node) error {
 	} else {
 		node.Status.NodeInfo.MachineID = info.MachineID
 		node.Status.NodeInfo.SystemUUID = info.SystemUUID
-		node.Status.Capacity = CapacityFromMachineInfo(info)
+		node.Status.Capacity = CapacityFromMachineInfo(info, kl.resourceMultipliers)
 		node.Status.Capacity[api.ResourcePods] = *resource.NewQuantity(
 			int64(kl.pods), resource.DecimalSI)
 		if node.Status.NodeInfo.BootID != "" &&
@@ -2425,6 +2431,8 @@ func (kl *Kubelet) setNodeStatus(node *api.Node) error {
 		node.Status.NodeInfo.BootID = info.BootID
 	}
 
+	glog.V(4).Infof("Kubelet node status: %v\n", node.Status)
+
 	verinfo, err := kl.cadvisor.VersionInfo()
 	if err != nil {
 		glog.Errorf("Error getting version info: %v", err)
diff --git a/pkg/kubelet/kubelet_test.go b/pkg/kubelet/kubelet_test.go
index 7777814..068b147 100644
--- a/pkg/kubelet/kubelet_test.go
+++ b/pkg/kubelet/kubelet_test.go
@@ -131,7 +131,7 @@ func newTestKubelet(t *testing.T) *TestKubelet {
 		t:            t,
 	}
 	kubelet.volumeManager = newVolumeManager()
-	kubelet.containerManager, _ = newContainerManager(fakeContainerMgrMountInt(), mockCadvisor, "", "", "")
+	kubelet.containerManager, _ = newContainerManager(fakeContainerMgrMountInt(), mockCadvisor, "", "", "", api.ResourceMultipliers{CPUMultiplier: 1.0, MemoryMultiplier: 1.0})
 	kubelet.networkConfigured = true
 	fakeClock := &util.FakeClock{Time: time.Now()}
 	kubelet.backOff = util.NewBackOff(time.Second, time.Minute)
diff --git a/pkg/kubelet/runonce_test.go b/pkg/kubelet/runonce_test.go
index 5d36d70..fe06e1f 100644
--- a/pkg/kubelet/runonce_test.go
+++ b/pkg/kubelet/runonce_test.go
@@ -51,7 +51,7 @@ func TestRunOnce(t *testing.T) {
 		diskSpaceManager:    diskSpaceManager,
 		containerRuntime:    fakeRuntime,
 	}
-	kb.containerManager, _ = newContainerManager(fakeContainerMgrMountInt(), cadvisor, "", "", "")
+	kb.containerManager, _ = newContainerManager(fakeContainerMgrMountInt(), cadvisor, "", "", "", api.ResourceMultipliers{CPUMultiplier: 1.0, MemoryMultiplier: 1.0})
 
 	kb.networkPlugin, _ = network.InitNetworkPlugin([]network.NetworkPlugin{}, "", network.NewFakeHost(nil))
 	if err := kb.setupDataDirs(); err != nil {
diff --git a/pkg/kubelet/util.go b/pkg/kubelet/util.go
index 1c2ab0a..8955516 100644
--- a/pkg/kubelet/util.go
+++ b/pkg/kubelet/util.go
@@ -26,13 +26,13 @@ import (
 	"k8s.io/kubernetes/pkg/securitycontext"
 )
 
-func CapacityFromMachineInfo(info *cadvisorApi.MachineInfo) api.ResourceList {
+func CapacityFromMachineInfo(info *cadvisorApi.MachineInfo, multipliers api.ResourceMultipliers) api.ResourceList {
 	c := api.ResourceList{
 		api.ResourceCPU: *resource.NewMilliQuantity(
-			int64(info.NumCores*1000),
+			int64(float32(info.NumCores*1000)*multipliers.CPUMultiplier),
 			resource.DecimalSI),
 		api.ResourceMemory: *resource.NewQuantity(
-			info.MemoryCapacity,
+			int64(float32(info.MemoryCapacity)*multipliers.MemoryMultiplier),
 			resource.BinarySI),
 	}
 	return c
